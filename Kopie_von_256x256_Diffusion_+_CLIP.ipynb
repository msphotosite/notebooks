{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGJahek5Xoqi"
   },
   "source": [
    "#**INSTRUCTIONS:**\n",
    "\n",
    "###1) Press Play Button in This First Code Block to download everything (Hover over code block to see Play Button)\n",
    "\n",
    "###2) Enter Text and Number of Images you want to generate\n",
    "\n",
    "###3) Run The Art Generator! \n",
    "\n",
    "V1.0.1\n",
    "\n",
    "**Found out the free version of Colab encounters issues with 512x512, so I updated this notebook to be 256x256 and made another if you wanted to generate 512x512! Here it is!** \n",
    "\n",
    "https://colab.research.google.com/drive/14GYrRHelCa3Gr5Qh4vc-h4Saxc-Ut2J9?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GwQ1f5H2zqBP"
   },
   "source": [
    "#**1)** Download everything required for Art Generator ‚ùó‚ùó‚ùó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "qZ3rNuAWAewx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check the GPU status\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yZsjzwS0YGo6"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "\n",
    "#@title Choose model here:\n",
    "diffusion_model = \"256x256_diffusion_uncond\" #@param [\"256x256_diffusion_uncond\", \"512x512_diffusion_uncond_finetune_008100\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "mQE-fIMnYKYK"
   },
   "outputs": [],
   "source": [
    "#@title Download diffusion model\n",
    "#!mkdir generations\n",
    "model_path = './content/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CoEypp3b1hUr"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "#!git clone https://github.com/openai/CLIP\n",
    "#!git clone https://github.com/crowsonkb/guided-diffusion\n",
    "#!pip install -e ./CLIP\n",
    "#!pip install -e ./guided-diffusion\n",
    "#!pip install lpips datetime\n",
    "#!pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Rl8RO5oz1hUr"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "import gc\n",
    "import io\n",
    "import math\n",
    "import sys\n",
    "from IPython import display\n",
    "import lpips\n",
    "from PIL import Image, ImageOps\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "from tqdm.notebook import tqdm\n",
    "sys.path.append('./CLIP')\n",
    "sys.path.append('./guided-diffusion')\n",
    "import clip\n",
    "from guided_diffusion.script_util import create_model_and_diffusion, model_and_diffusion_defaults\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dn9Kq_tv1l2J"
   },
   "outputs": [],
   "source": [
    "# https://gist.github.com/adefossez/0646dbe9ed4005480a2407c62aac8869\n",
    "\n",
    "def interp(t):\n",
    "    return 3 * t**2 - 2 * t ** 3\n",
    "\n",
    "def perlin(width, height, scale=10, device=None):\n",
    "    gx, gy = torch.randn(2, width + 1, height + 1, 1, 1, device=device)\n",
    "    xs = torch.linspace(0, 1, scale + 1)[:-1, None].to(device)\n",
    "    ys = torch.linspace(0, 1, scale + 1)[None, :-1].to(device)\n",
    "    wx = 1 - interp(xs)\n",
    "    wy = 1 - interp(ys)\n",
    "    dots = 0\n",
    "    dots += wx * wy * (gx[:-1, :-1] * xs + gy[:-1, :-1] * ys)\n",
    "    dots += (1 - wx) * wy * (-gx[1:, :-1] * (1 - xs) + gy[1:, :-1] * ys)\n",
    "    dots += wx * (1 - wy) * (gx[:-1, 1:] * xs - gy[:-1, 1:] * (1 - ys))\n",
    "    dots += (1 - wx) * (1 - wy) * (-gx[1:, 1:] * (1 - xs) - gy[1:, 1:] * (1 - ys))\n",
    "    return dots.permute(0, 2, 1, 3).contiguous().view(width * scale, height * scale)\n",
    "\n",
    "def perlin_ms(octaves, width, height, grayscale, device=device):\n",
    "    out_array = [0.5] if grayscale else [0.5, 0.5, 0.5]\n",
    "    # out_array = [0.0] if grayscale else [0.0, 0.0, 0.0]\n",
    "    for i in range(1 if grayscale else 3):\n",
    "        scale = 2 ** len(octaves)\n",
    "        oct_width = width\n",
    "        oct_height = height\n",
    "        for oct in octaves:\n",
    "            p = perlin(oct_width, oct_height, scale, device)\n",
    "            out_array[i] += p * oct\n",
    "            scale //= 2\n",
    "            oct_width *= 2\n",
    "            oct_height *= 2\n",
    "    return torch.cat(out_array)\n",
    "\n",
    "def create_perlin_noise(octaves=[1, 1, 1, 1], width=2, height=2, grayscale=True):\n",
    "    out = perlin_ms(octaves, width, height, grayscale)\n",
    "    if grayscale:\n",
    "        out = TF.resize(size=(side_y, side_x), img=out.unsqueeze(0))\n",
    "        out = TF.to_pil_image(out.clamp(0, 1)).convert('RGB')\n",
    "    else:\n",
    "        out = out.reshape(-1, 3, out.shape[0]//3, out.shape[1])\n",
    "        out = TF.resize(size=(side_y, side_x), img=out)\n",
    "        out = TF.to_pil_image(out.clamp(0, 1).squeeze())\n",
    "\n",
    "    out = ImageOps.autocontrast(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "m4xKFJ9i1l2K"
   },
   "outputs": [],
   "source": [
    "def fetch(url_or_path):\n",
    "    if str(url_or_path).startswith('http://') or str(url_or_path).startswith('https://'):\n",
    "        r = requests.get(url_or_path)\n",
    "        r.raise_for_status()\n",
    "        fd = io.BytesIO()\n",
    "        fd.write(r.content)\n",
    "        fd.seek(0)\n",
    "        return fd\n",
    "    return open(url_or_path, 'rb')\n",
    "\n",
    "\n",
    "def parse_prompt(prompt):\n",
    "    if prompt.startswith('http://') or prompt.startswith('https://'):\n",
    "        vals = prompt.rsplit(':', 2)\n",
    "        vals = [vals[0] + ':' + vals[1], *vals[2:]]\n",
    "    else:\n",
    "        vals = prompt.rsplit(':', 1)\n",
    "    vals = vals + ['', '1'][len(vals):]\n",
    "    return vals[0], float(vals[1])\n",
    "\n",
    "def sinc(x):\n",
    "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
    "\n",
    "def lanczos(x, a):\n",
    "    cond = torch.logical_and(-a < x, x < a)\n",
    "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
    "    return out / out.sum()\n",
    "\n",
    "def ramp(ratio, width):\n",
    "    n = math.ceil(width / ratio + 1)\n",
    "    out = torch.empty([n])\n",
    "    cur = 0\n",
    "    for i in range(out.shape[0]):\n",
    "        out[i] = cur\n",
    "        cur += ratio\n",
    "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
    "\n",
    "def resample(input, size, align_corners=True):\n",
    "    n, c, h, w = input.shape\n",
    "    dh, dw = size\n",
    "\n",
    "    input = input.reshape([n * c, 1, h, w])\n",
    "\n",
    "    if dh < h:\n",
    "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
    "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
    "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
    "\n",
    "    if dw < w:\n",
    "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
    "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
    "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
    "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
    "\n",
    "    input = input.reshape([n, c, h, w])\n",
    "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
    "\n",
    "class MakeCutouts(nn.Module):\n",
    "    def __init__(self, cut_size, cutn, skip_augs=False):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.skip_augs = skip_augs\n",
    "        self.augs = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomAffine(degrees=15, translate=(0.1, 0.1), interpolation=TF.InterpolationMode.BILINEAR),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomPerspective(distortion_scale=0.4, p=0.7),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            T.Lambda(lambda x: x + torch.randn_like(x) * 0.01),\n",
    "            T.RandomGrayscale(p=0.35),\n",
    "        ])\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = T.Pad(input.shape[2]//4, fill=0)(input)\n",
    "        sideY, sideX = input.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "\n",
    "        cutouts = []\n",
    "        for ch in range(cutn):\n",
    "            if ch > cutn - cutn//4:\n",
    "                cutout = input.clone()\n",
    "            else:\n",
    "                size = int(max_size * torch.zeros(1,).normal_(mean=.8, std=.3).clip(float(self.cut_size/max_size), 1.))\n",
    "                offsetx = torch.randint(0, abs(sideX - size + 1), ())\n",
    "                offsety = torch.randint(0, abs(sideY - size + 1), ())\n",
    "                cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "\n",
    "            if not self.skip_augs:\n",
    "                cutout = self.augs(cutout)\n",
    "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
    "            del cutout\n",
    "\n",
    "        cutouts = torch.cat(cutouts, dim=0)\n",
    "        return cutouts\n",
    "\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "\n",
    "def tv_loss(input):\n",
    "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
    "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
    "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
    "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
    "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
    "\n",
    "\n",
    "def range_loss(input):\n",
    "    return (input - input.clamp(-1, 1)).pow(2).mean([1, 2, 3])\n",
    "\n",
    "def unitwise_norm(x, norm_type=2.0):\n",
    "    if x.ndim <= 1:\n",
    "        return x.norm(norm_type)\n",
    "    else:\n",
    "        # works for nn.ConvNd and nn,Linear where output dim is first in the kernel/weight tensor\n",
    "        # might need special cases for other weights (possibly MHA) where this may not be true\n",
    "        return x.norm(norm_type, dim=tuple(range(1, x.ndim)), keepdim=True)\n",
    "\n",
    "def adaptive_clip_grad(parameters, clip_factor=0.01, eps=1e-3, norm_type=2.0):\n",
    "    if isinstance(parameters, torch.Tensor):\n",
    "        parameters = [parameters]\n",
    "    for p in parameters:\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        p_data = p.detach()\n",
    "        g_data = p.grad.detach()\n",
    "        max_norm = unitwise_norm(p_data, norm_type=norm_type).clamp_(min=eps).mul_(clip_factor)\n",
    "        grad_norm = unitwise_norm(g_data, norm_type=norm_type)\n",
    "        clipped_grad = g_data * (max_norm / grad_norm.clamp(min=1e-6))\n",
    "        new_grads = torch.where(grad_norm < max_norm, g_data, clipped_grad)\n",
    "        p.grad.detach().copy_(new_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "iWYC7LXF1l2L"
   },
   "outputs": [],
   "source": [
    "def regen_perlin():\n",
    "    if perlin_mode == 'color':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, False)\n",
    "    elif perlin_mode == 'gray':\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, True)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "    else:\n",
    "        init = create_perlin_noise([1.5**-i*0.5 for i in range(12)], 1, 1, False)\n",
    "        init2 = create_perlin_noise([1.5**-i*0.5 for i in range(8)], 4, 4, True)\n",
    "\n",
    "    init = TF.to_tensor(init).add(TF.to_tensor(init2)).div(2).to(device).unsqueeze(0).mul(2).sub(1)\n",
    "    del init2\n",
    "    return init.expand(batch_size, -1, -1, -1)\n",
    "\n",
    "def do_run(x):\n",
    "    loss_values = []\n",
    " \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # torch.backends.cudnn.deterministic = True\n",
    " \n",
    "    make_cutouts = MakeCutouts(clip_size, cutn, skip_augs=skip_augs)\n",
    "    target_embeds, weights = [], []\n",
    " \n",
    "    for prompt in text_prompts:\n",
    "        txt, weight = parse_prompt(prompt)\n",
    "        txt = clip_model.encode_text(clip.tokenize(prompt).to(device)).float()\n",
    "        target_embeds.append(txt)\n",
    "        weights.append(weight)\n",
    " \n",
    "    for prompt in image_prompts:\n",
    "        path, weight = parse_prompt(prompt)\n",
    "        img = Image.open(fetch(path)).convert('RGB')\n",
    "        img = TF.resize(img, min(side_x, side_y, *img.size), T.InterpolationMode.LANCZOS)\n",
    "        batch = make_cutouts(TF.to_tensor(img).to(device).unsqueeze(0).mul(2).sub(1))\n",
    "        embed = clip_model.encode_image(normalize(batch)).float()\n",
    "        target_embeds.append(embed)\n",
    "        weights.extend([weight / cutn] * cutn)\n",
    " \n",
    "    target_embeds = torch.cat(target_embeds)\n",
    "    weights = torch.tensor(weights, device=device)\n",
    "    if weights.sum().abs() < 1e-3:\n",
    "        raise RuntimeError('The weights must not sum to 0.')\n",
    "    weights /= weights.sum().abs()\n",
    " \n",
    "    init = None\n",
    "    if init_image is not None:\n",
    "        init = Image.open(fetch(init_image)).convert('RGB')\n",
    "        init = init.resize((side_x, side_y), Image.LANCZOS)\n",
    "        init = TF.to_tensor(init).unsqueeze(0).expand(batch_size, -1, -1, -1).to(device).mul(2).sub(1)\n",
    " \n",
    "    cur_t = None\n",
    "    def cond_fn(x, t, out, y=None):\n",
    "        n = x.shape[0]\n",
    "        fac = diffusion.sqrt_one_minus_alphas_cumprod[cur_t]\n",
    "        x_in = out['pred_xstart'] * fac + x * (1 - fac)\n",
    "        x_in_grad = torch.zeros_like(x_in)\n",
    "\n",
    "        for i in range(cutn_batches):\n",
    "            clip_in = normalize(make_cutouts(x_in.add(1).div(2)))\n",
    "            image_embeds = clip_model.encode_image(clip_in).float()\n",
    "            dists = spherical_dist_loss(image_embeds.unsqueeze(1), target_embeds.unsqueeze(0))\n",
    "            dists = dists.view([cutn, n, -1])\n",
    "            losses = dists.mul(weights).sum(2).mean(0)\n",
    "            loss_values.append(losses.sum().item())\n",
    "            x_in_grad += torch.autograd.grad(losses.sum() * clip_guidance_scale, x_in)[0] / cutn_batches\n",
    "\n",
    "        tv_losses = tv_loss(x_in)\n",
    "        range_losses = range_loss(out['pred_xstart'])\n",
    "        sat_losses = torch.abs(x_in - x_in.clamp(min=-1,max=1)).mean()\n",
    "        loss = tv_losses.sum() * tv_scale + range_losses.sum() * range_scale + sat_losses.sum() * sat_scale\n",
    "        if init is not None and init_scale:\n",
    "            init_losses = lpips_model(x_in, init)\n",
    "            loss = loss + init_losses.sum() * init_scale\n",
    "        x_in_grad += torch.autograd.grad(loss, x_in)[0]\n",
    "        grad = -torch.autograd.grad(x_in, x, x_in_grad)[0]\n",
    "        adaptive_clip_grad([x])\n",
    "        magnitude = grad.square().mean().sqrt()\n",
    "        return grad * magnitude.clamp(max=clamp_max) / magnitude\n",
    " \n",
    "    if model_config['timestep_respacing'].startswith('ddim'):\n",
    "        sample_fn = diffusion.ddim_sample_loop_progressive\n",
    "    else:\n",
    "        sample_fn = diffusion.p_sample_loop_progressive\n",
    " \n",
    "    original_target_embeds = target_embeds.clone()\n",
    "    for i in range(n_batches):\n",
    "        cur_t = diffusion.num_timesteps - skip_timesteps - 1\n",
    "\n",
    "        if fuzzy_prompt:\n",
    "            target_embeds = original_target_embeds.clone() +  torch.randn_like(target_embeds).cuda() * rand_mag\n",
    "\n",
    "        if perlin_init:\n",
    "            init = regen_perlin()\n",
    " \n",
    "        if model_config['timestep_respacing'].startswith('ddim'):\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, side_y, side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_timesteps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                eta=eta,\n",
    "                cond_fn_with_grad=True,\n",
    "            )\n",
    "        else:\n",
    "            samples = sample_fn(\n",
    "                model,\n",
    "                (batch_size, 3, side_y, side_x),\n",
    "                clip_denoised=clip_denoised,\n",
    "                model_kwargs={},\n",
    "                cond_fn=cond_fn,\n",
    "                progress=True,\n",
    "                skip_timesteps=skip_timesteps,\n",
    "                init_image=init,\n",
    "                randomize_class=randomize_class,\n",
    "                cond_fn_with_grad=True,\n",
    "            )\n",
    "\n",
    "        for j, sample in enumerate(samples):\n",
    "            # display.clear_output(wait=True)\n",
    "            cur_t -= 1\n",
    "            if j % display_rate == 0 or cur_t == -1:\n",
    "                for k, image in enumerate(sample['pred_xstart']):\n",
    "                    tqdm.write(f'Generated Image {x}, step {j}:')\n",
    "                    current_time = datetime.now().strftime('%H:%M:%S')\n",
    "                    filename = f'generations/generation_{x}.png'\n",
    "                    image = TF.to_pil_image(image.add(1).div(2).clamp(0, 1))\n",
    "                    image.save('/content/' + filename)\n",
    "                    display.display(display.Image('/content/' + filename))\n",
    "                    if google_drive and cur_t == -1:\n",
    "                        image.save('/content/drive/MyDrive/' + filename)\n",
    " \n",
    "        plt.plot(np.array(loss_values), 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Fpbody2NCR7w"
   },
   "outputs": [],
   "source": [
    "# timestep_respacing = 'ddim50' # Modify this value to decrease the number of timesteps.\n",
    "timestep_respacing = '50'\n",
    "diffusion_steps = 1000\n",
    "\n",
    "model_config = model_and_diffusion_defaults()\n",
    "if diffusion_model == '512x512_diffusion_uncond_finetune_008100':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': timestep_respacing,\n",
    "        'image_size': 512,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "        'use_checkpoint': True,\n",
    "    })\n",
    "elif diffusion_model == '256x256_diffusion_uncond':\n",
    "    model_config.update({\n",
    "        'attention_resolutions': '32, 16, 8',\n",
    "        'class_cond': False,\n",
    "        'diffusion_steps': diffusion_steps,\n",
    "        'rescale_timesteps': True,\n",
    "        'timestep_respacing': timestep_respacing,\n",
    "        'image_size': 256,\n",
    "        'learn_sigma': True,\n",
    "        'noise_schedule': 'linear',\n",
    "        'num_channels': 256,\n",
    "        'num_head_channels': 64,\n",
    "        'num_res_blocks': 2,\n",
    "        'resblock_updown': True,\n",
    "        'use_fp16': True,\n",
    "        'use_scale_shift_norm': True,\n",
    "        'use_checkpoint': True,\n",
    "    })\n",
    "side_x = side_y = model_config['image_size']\n",
    "\n",
    "model, diffusion = create_model_and_diffusion(**model_config)\n",
    "model.load_state_dict(torch.load(f'{model_path}{diffusion_model}.pt', map_location='cpu'))\n",
    "model.requires_grad_(False).eval().to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'qkv' in name or 'norm' in name or 'proj' in name:\n",
    "        param.requires_grad_()\n",
    "if model_config['use_fp16']:\n",
    "    model.convert_to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VnQjGugaDZPJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: C:\\Users\\Manuela\\AppData\\Roaming\\Python\\Python39\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n"
     ]
    }
   ],
   "source": [
    "clip_model = clip.load('ViT-B/16', jit=False)[0].eval().requires_grad_(False).to(device)\n",
    "clip_size = clip_model.visual.input_resolution\n",
    "normalize = T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "lpips_model = lpips.LPIPS(net='vgg').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zY-8I90LkC6"
   },
   "source": [
    "# **2)** Parameters for The Art Generator ü§ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "U0PwzFZbLfcy"
   },
   "outputs": [],
   "source": [
    "text = \"a beautiful watercolor painting of wind\" #@param {type:\"string\"}\n",
    "number_of_images = 5 #@param {type: \"number\"}\n",
    "# eta = 0.5#@param {type: \"number\"}\n",
    "text_prompts = [\n",
    "    text\n",
    "]\n",
    "\n",
    "image_prompts = [\n",
    "    # 'mona.jpg',\n",
    "]\n",
    "\n",
    "clip_guidance_scale = 15000 # 5000 (new:15000) - Controls how much the image should look like the prompt.\n",
    "tv_scale = 2500 # 500 (new:2500) - Controls the smoothness of the final output.\n",
    "range_scale = 100 # 100 - Controls how far out of range RGB values are allowed to be.\n",
    "sat_scale = 0 # 0 - Controls how much saturation is allowed. From nshepperd's JAX notebook, though not sure if it's doing anything right now...\n",
    "cutn = 16 # 16 - Controls how many crops to take from the image. Increase for higher quality.\n",
    "cutn_batches = 2 # 2 - Accumulate CLIP gradient from multiple batches of cuts [Can help with OOM errors / Low VRAM]\n",
    "\n",
    "init_image = None # None - URL or local path\n",
    "init_scale = 0 # 0 - This enhances the effect of the init image, a good value is 1000\n",
    "skip_timesteps = 8 # 10 (new:5) - Controls the starting point along the diffusion timesteps\n",
    "\n",
    "# Try this option for random natural-looking noise in place of an init image:\n",
    "perlin_init = True # False - Option to start with random perlin noise\n",
    "perlin_mode = 'mixed' # 'mixed' ('gray', 'color')\n",
    "if init_image is not None: # Can't combine init_image and perlin options\n",
    "  perlin_init = False\n",
    "\n",
    "skip_augs = False # False - Controls whether to skip torchvision augmentations\n",
    "randomize_class = True # True - Controls whether the imagenet class is randomly changed each iteration\n",
    "clip_denoised = False # False - Determines whether CLIP discriminates a noisy or denoised image\n",
    "clamp_max = 0.035 # 0.05 (new:0.035)\n",
    "\n",
    "fuzzy_prompt = False # False - Controls whether to add multiple noisy prompts to the prompt losses\n",
    "rand_mag = 0.05 # 0.05 - Controls the magnitude of the random noise\n",
    "eta = 0.5 # 0.5 - DDIM hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nf9hTc8YLoLx"
   },
   "source": [
    "# **3)** Generate Art üòé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "form",
    "id": "LHLiO56OfwgD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662cbf6b07884dc5a44c0a7b47fba9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 3286771369\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"unfolded2d_copy\" not implemented for 'Half'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10340/926144685.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m       \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       \u001b[0mdo_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10340/955953398.py\u001b[0m in \u001b[0;36mdo_run\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    126\u001b[0m             )\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;31m# display.clear_output(wait=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mcur_t\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample_loop_progressive\u001b[1;34m(self, model, shape, noise, clip_denoised, denoised_fn, cond_fn, model_kwargs, device, progress, skip_timesteps, init_image, randomize_class, cond_fn_with_grad)\u001b[0m\n\u001b[0;32m    635\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                 \u001b[0msample_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_sample_with_grad\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcond_fn_with_grad\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m                 out = sample_fn(\n\u001b[0m\u001b[0;32m    638\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m                     \u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_sample_with_grad\u001b[1;34m(self, model, x, t, clip_denoised, denoised_fn, cond_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m             out = self.p_mean_variance(\n\u001b[0m\u001b[0;32m    510\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\respace.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[1;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[1;32m---> 91\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp_mean_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     def training_losses(\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\gaussian_diffusion.py\u001b[0m in \u001b[0;36mp_mean_variance\u001b[1;34m(self, model, x, t, clip_denoised, denoised_fn, model_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m         \u001b[0mmodel_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scale_timesteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_var_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mModelVarType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModelVarType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLEARNED_RANGE\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\respace.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, ts, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrescale_timesteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m             \u001b[0mnew_ts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1000.0\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_num_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_ts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\unet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, timesteps, y)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_blocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m             \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m             \u001b[0mhs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmiddle_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\manuela\\downloads\\notebooks\\guided-diffusion\\guided_diffusion\\unet.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, emb)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"unfolded2d_copy\" not implemented for 'Half'"
     ]
    }
   ],
   "source": [
    "#@title Run This! üèé\n",
    "display_rate = 1\n",
    "n_batches = 1 # 1 - Controls how many consecutive batches of images are generated\n",
    "batch_size = 1 # 1 - Controls how many images are generated in parallel in a batch\n",
    "# number_of_images = 5\n",
    "\n",
    "# seed = 0\n",
    "# seed = random.randint(0, 2**32) # Choose a random seed and print it at end of run for reproduction\n",
    "for x in range(0, number_of_images):\n",
    "  # display.clear_output(wait=True)\n",
    "  seed = random.randint(0, 2**32)\n",
    "  try:\n",
    "      gc.collect()\n",
    "      torch.cuda.empty_cache()\n",
    "      do_run(x)\n",
    "  except KeyboardInterrupt:\n",
    "      pass\n",
    "  finally:\n",
    "      print('seed', seed)\n",
    "      gc.collect()\n",
    "      torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "4sjiHy2ygOJX"
   },
   "outputs": [],
   "source": [
    "#@title Plot Generated Images! üéØ\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "images = []\n",
    "for img_path in glob.glob('./generations/*.png'):\n",
    "    images.append(mpimg.imread(img_path))\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "columns = 5\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwL-U6Nt2q5h"
   },
   "source": [
    "#üåüüåüüåü **Beautiful Prompts you should try!** üåüüåüüåü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4k_7QguF2PPV"
   },
   "source": [
    "    the universe is a glitch by greg rutkowski\n",
    "    a space nebula rendered in Cinema4D\n",
    "    revolution of the souls, vector art\n",
    "    the rise of consciousness in the style of WPAP\n",
    "    a vaporwave dragon breathing fire by ross tran\n",
    "    \"Cognitive Transcendence\", matte painting trending on artstation\n",
    "    mirror of love, by greg rutkowski and james jean\n",
    "    falling forever through a bottomless abyss speckled with stars. painting by greg rutkowski\n",
    "    a beautiful epic fantasy painting of a giant robot\n",
    "    the fire of the mind by Ross Tran\n",
    "    the first day of the heavens! trending on artstation\n",
    "    Garden of Hesperides by ArtStation\n",
    "    a beautiful epic wondrous fantasy painting of the ocean\n",
    "    a beautiful watercolor painting of wind\n",
    "    a tropical landscape by Ivan Aivazovsky\n",
    "    a dramatic mountainous landscape by Ivan Aivazovsky\n",
    "    the aurora at night by Ivan Aivazovsky\n",
    "    a painting of a witch brewing a Halloween potion by Greg Rutkowski\n",
    "    a surreal wizards tower by Casper David\n",
    "    a beautiful epic fantasy painting of a giant robot\n",
    "    a rainy city street in the style of cyberpunk noir\n",
    "    the Tower of Babel by Thomas Kinkade\n",
    "    wings of angelic fire in the darkness, trending on artstation\n",
    "    a beautiful fantasy land forest, trending on ArtStation\n",
    "    a desert landscape by Ivan Aivazovsky\n",
    "    a heavenly cloud city by Greg Rutkowski\n",
    "    wings of angelic fire in the darkness, trending on artstation\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASg7_ZeOzkt3"
   },
   "source": [
    "-\n",
    "#**Want to Super-Enhance the Generated Image with another advanced A.I.? Enhances a 500x500 image to 1760x1760 with no quality loss!!!**: \n",
    "\n",
    "-\n",
    "\n",
    "https://colab.research.google.com/drive/1O8kr-iyRll5eQsaOH-vQIc3ImEKWsvF-?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "GwQ1f5H2zqBP"
   ],
   "machine_shape": "hm",
   "name": "Kopie von 256x256 Diffusion + CLIP.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
